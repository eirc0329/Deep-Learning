{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 21:59:29.311329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.326590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.326771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.327428: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 21:59:29.327771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.327945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.328087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.635027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.635202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.635337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:59:29.635466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11372 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "\n",
    "with open('./words_captcha/spec_train_val.txt') as fin:\n",
    "    for line in fin:\n",
    "        image_name, caption = line.strip().split()\n",
    "        all_img_name_vector.append(f'./words_captcha/{image_name}.png')\n",
    "        all_captions.append('<start> ' + ' '.join(caption) + ' <end>')\n",
    "\n",
    "test_img_name = set(glob.glob(f'./words_captcha/*.png')) - set(all_img_name_vector)\n",
    "all_img_name_vector += sorted(test_img_name)\n",
    "\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                  oov_token='')\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "cap_seqs = tokenizer.texts_to_sequences(all_captions)\n",
    "\n",
    "cap_seqs = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs, padding='post')\n",
    "max_length = len(cap_seqs[0])\n",
    "\n",
    "\n",
    "\n",
    "num_train = 100000\n",
    "num_valid = 20000\n",
    "img_name_train, img_name_valid, img_name_test  = all_img_name_vector[:num_train], all_img_name_vector[num_train:num_train+num_valid], all_img_name_vector[num_train+num_valid:]\n",
    "train_seqs, valid_seqs = cap_seqs[:num_train], cap_seqs[num_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 120000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seqs), len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (160, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img = tf.io.read_file(img_name)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = img / 255 * 2 - 1\n",
    "    return img, cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, train_seqs))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((img_name_valid, valid_seqs))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 15, embedding_dim)\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 15, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 15, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector == (batch_size, embedding_dim)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class conv_leaky_relu(layers.Layer):\n",
    "    def __init__(self, filters, size, stride):\n",
    "        super(conv_leaky_relu, self).__init__()\n",
    "        self.conv_2d = layers.Conv2D(filters, size, stride, padding='same')\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "        self.leakey_relu = layers.LeakyReLU(0.1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_2d(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.leakey_relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"YOLO\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 160, 300, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu (conv_leaky_ (None, 80, 150, 64)       9728      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 40, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_1 (conv_leak (None, 40, 75, 192)       111552    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 37, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_2 (conv_leak (None, 20, 37, 128)       25216     \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_3 (conv_leak (None, 20, 37, 256)       296192    \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_4 (conv_leak (None, 20, 37, 256)       66816     \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_5 (conv_leak (None, 20, 37, 512)       1182208   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_6 (conv_leak (None, 10, 18, 256)       132352    \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_7 (conv_leak (None, 10, 18, 512)       1182208   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_8 (conv_leak (None, 10, 18, 256)       132352    \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_9 (conv_leak (None, 10, 18, 512)       1182208   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_10 (conv_lea (None, 10, 18, 256)       132352    \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_11 (conv_lea (None, 10, 18, 512)       1182208   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_12 (conv_lea (None, 10, 18, 256)       132352    \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_13 (conv_lea (None, 10, 18, 512)       1182208   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_14 (conv_lea (None, 10, 18, 512)       264704    \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_15 (conv_lea (None, 10, 18, 1024)      4723712   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 9, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_16 (conv_lea (None, 5, 9, 512)         526848    \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_17 (conv_lea (None, 5, 9, 1024)        4723712   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_18 (conv_lea (None, 5, 9, 512)         526848    \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_19 (conv_lea (None, 5, 9, 1024)        4723712   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_20 (conv_lea (None, 5, 9, 1024)        9442304   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_21 (conv_lea (None, 3, 5, 1024)        9442304   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_22 (conv_lea (None, 3, 5, 1024)        9442304   \n",
      "_________________________________________________________________\n",
      "conv_leaky_relu_23 (conv_lea (None, 3, 5, 1024)        9442304   \n",
      "=================================================================\n",
      "Total params: 60,208,704\n",
      "Trainable params: 60,182,336\n",
      "Non-trainable params: 26,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, layers, Model\n",
    "\n",
    "inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "x = conv_leaky_relu(64, 7, 2)(inputs)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(192, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(128, 1, 1)(x)\n",
    "x = conv_leaky_relu(256, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(256, 1, 1)(x)\n",
    "x = conv_leaky_relu(512, 3, 1)(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(512, 1, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "x = conv_leaky_relu(1024, 3, 2)(x)\n",
    "x = conv_leaky_relu(1024, 3, 1)(x)\n",
    "outputs = conv_leaky_relu(1024, 3, 1)(x)\n",
    "\n",
    "feature_extractor = Model(inputs=inputs, outputs=outputs, name='YOLO')\n",
    "feature_extractor.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape after passing through fc == (batch_size, 15, embedding_dim)\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # x shape == (batch_size, 1)\n",
    "        # features shape == (batch_size, 15, embedding_dim)\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "\n",
    "        # context_vector shape == (batch_size, embedding_dim)\n",
    "        # attention_weights shape == (batch_size, 15, 1)\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output shape == (batch_size, 1, hidden_size)\n",
    "        # state(hidden) shape == (batch_size, hidden_size)\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # x shape == (batch_size, 1, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(5e-5)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/train/'\n",
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extractor(img_tensor, True)\n",
    "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = feature_extractor.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]2023-11-28 21:59:33.801286: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-11-28 21:59:39.969663: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8500\n",
      "2023-11-28 21:59:40.097168: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-11-28 21:59:42.945305: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2023-11-28 21:59:42.990048: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.40GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "100%|██████████| 1000/1000 [05:25<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.680886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 0.643459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 0.127397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 0.051217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 0.030278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 0.021954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss 0.017864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 0.015396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss 0.012178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:20<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 0.010023\n",
      "Time taken for 10 epoch 3245.447103738785 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (img_tensor, target)) in enumerate(tqdm(dataset_train, total=num_steps)):\n",
    "        batch_loss, t_loss= train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps)) \n",
    "\n",
    "print('Time taken for {} epoch {} sec\\n'.format(EPOCHS - start_epoch, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims(\n",
    "        [tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    features = feature_extractor(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    for _ in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f01a35cd970>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt.restore('./checkpoints/train/ckpt-10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func2(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = img / 255 * 2 - 1\n",
    "    return img, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((img_name_test))\\\n",
    "                              .map(map_func2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                              .batch(100)\\\n",
    "                              .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Lab12-2_112062559.txt', 'w') as f:\n",
    "    for step, (img_tensor, img_path) in enumerate(dataset_test):\n",
    "        predictions = []\n",
    "        for seq in predict(img_tensor).numpy():\n",
    "            result = ''\n",
    "            for s in seq[1:]:\n",
    "                if s == tokenizer.word_index['<end>']:\n",
    "                    break\n",
    "                result += tokenizer.index_word[s]\n",
    "            predictions.append(result)\n",
    "\n",
    "        for path, pred in zip(img_path, predictions):\n",
    "            path = path.numpy().decode('utf-8')\n",
    "            name = re.search('(a[0-9]+)', path).group(1)\n",
    "            f.write(f'{name} {pred}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy : 0.952\n"
     ]
    }
   ],
   "source": [
    "total_num = 0\n",
    "correct_num = 0\n",
    "for img_tensor, target in dataset_valid:\n",
    "    predictions = []\n",
    "    for seq in predict(img_tensor).numpy():\n",
    "        result = ''\n",
    "        for s in seq[1:]:\n",
    "            if s == tokenizer.word_index['<end>']:\n",
    "                break\n",
    "            result += tokenizer.index_word[s]\n",
    "        predictions.append(result)\n",
    "\n",
    "    answers = []\n",
    "    for seq in target.numpy():\n",
    "        result = ''\n",
    "        for s in seq[1:]:\n",
    "            if s == tokenizer.word_index['<end>']:\n",
    "                break\n",
    "            result += tokenizer.index_word[s]\n",
    "        answers.append(result)\n",
    "\n",
    "    for pred, ans in zip(predictions, answers):\n",
    "        total_num += 1\n",
    "        if pred == ans:\n",
    "            correct_num += 1\n",
    "\n",
    "print(f'testing accuracy : {correct_num / total_num:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "我用了範例中的attention-based model,其中唯一不同的地方是feature extractor的地方把原本的inception_v3換成YOLO。總共訓練了10個EPOCHS後得到的testing accuracy是0.952\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
